{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import IPython.display\n",
    "\n",
    "import librosa\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from collections import Counter, OrderedDict\n",
    "from torchsummary import summary\n",
    "\n",
    "# load other modules --> repo root path\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import torch\n",
    "from utils import text, audio\n",
    "from utils.logging import Logger\n",
    "from params.params import Params as hp\n",
    "from modules.tacotron2 import Tacotron\n",
    "from dataset.dataset import TextToSpeechDataset, TextToSpeechDatasetCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.sample_rate = 22050\n",
    "hp.stft_window_ms = 50\n",
    "hp.stft_shift_ms = 12.5\n",
    "hp.num_fft = 1102\n",
    "hp.num_mels = 80\n",
    "hp.use_preemphasis = True\n",
    "\n",
    "waveform = audio.load(\"../data/ljspeech/wavs/LJ002-0001.wav\")\n",
    "\n",
    "melspec = audio.mel_spectrogram(waveform)\n",
    "spec = audio.spectrogram(waveform)\n",
    "\n",
    "Logger._plot_spectrogram(melspec);\n",
    "Logger._plot_spectrogram(spec);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=waveform, rate=hp.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.griffin_lim_iters = 60\n",
    "\n",
    "inverse_melspec = audio.inverse_mel_spectrogram(melspec)\n",
    "IPython.display.Audio(data=inverse_melspec, rate=hp.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.griffin_lim_iters = 60\n",
    "\n",
    "inverse_melspec = audio.inverse_spectrogram(spec)\n",
    "IPython.display.Audio(data=inverse_melspec, rate=hp.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dataparallel_prefix(state_dict): \n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:]\n",
    "        new_state_dict[name] = v\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(checkpoint):   \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    state = torch.load(checkpoint, map_location=device)\n",
    "    hp.load_state_dict(state['parameters'])\n",
    "    model = Tacotron()\n",
    "    model.load_state_dict(remove_dataparallel_prefix(state['model']))   \n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, inputs):\n",
    "    \n",
    "    inputs = [l.rstrip().split('|') for l in inputs if l]\n",
    "\n",
    "    spectrograms = []\n",
    "    for i in inputs:\n",
    "        t = torch.LongTensor(text.to_sequence(i[0], use_phonemes=hp.use_phonemes))\n",
    "        l = torch.LongTensor([hp.languages.index(i[2])]) if hp.multi_language else None\n",
    "        s = torch.LongTensor([hp.unique_speakers.index(i[1])]) if hp.multi_speaker else None\n",
    "\n",
    "        if torch.cuda.is_available(): \n",
    "            t = t.cuda(non_blocking=True)\n",
    "            if l: l = l.cuda(non_blocking=True)\n",
    "            if s: s = s.cuda(non_blocking=True)\n",
    "\n",
    "        spectrograms.append(model.inference(t, speaker=s, language=l).cpu().detach().numpy())\n",
    "\n",
    "    return spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"../checkpoints/FRGE-SEP_loss-89-0.143\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(checkpoint)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\"erlauben sie bitte, dass ich mich kurz vorstelle. ich heiße jana novakova.||german\",\n",
    "          \"les socialistes et les républicains sont venus apporter leurs voix à la majorité pour ce texte.||french\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_spectrograms = inference(model, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.griffin_lim_iters = 60\n",
    "hp.griffin_lim_power = 1.45\n",
    "\n",
    "for i, s in enumerate(generated_spectrograms):\n",
    "    s = audio.denormalize_spectrogram(s, not hp.predict_linear)\n",
    "    w = audio.inverse_spectrogram(s, not hp.predict_linear)\n",
    "    a = IPython.display.Audio(data=w, rate=hp.sample_rate)\n",
    "    IPython.display.display(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
