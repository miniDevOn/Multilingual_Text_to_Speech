<h1 align="center">Multi-lingual Speech Synthesis</h1>

<p align="center">
<a href="https://colab.research.google.com/github/Tomiinek/Multilingual_Text_to_Speech/blob/master/notebooks/code_switching_demo.ipynb"><b>Interactive synthesis demo</b></a> | 
<a href="http://tts.neqindi.cz/index.php?page=gallery"><b>Website with samples</b></a>
</p>

This repository contains an implementation of **Tacotron 2** that supports **multi-lingual experiments** and that implements different approaches to **encoder parameter sharing**. It combines ideas from [Learning to speak fluently in a foreign language: Multilingual speech synthesis and cross-language voice cloning](https://google.github.io/tacotron/publications/multilingual/index.html) and [End-to-End Code-Switched TTS with Mix of Monolingual Recordings](https://csttsdemo.github.io/).

In our work, we compared the abilities of the **three models**. The first **shares the whole encoder** and uses an **adversarial classifier** to remove language-dependent information. The second has **separate encoders** for each language and the third combines the best of both approaches, i.e. effective parameter sharing of the first method and flexibility of the second. It has an encoder with **language-specific parameters** generated by a **parameter generator network** and it also makes use of the adversarial process.

**Interactive demos** of code-switching abilities and multi-lingual training of our model trained on an enhanced CSS10 dataset are [here](https://colab.research.google.com/github/Tomiinek/Multilingual_Text_to_Speech/blob/master/notebooks/code_switching_demo.ipynb) and [here](https://github.com/Tomiinek/Multilingual_Text_to_Speech/blob/master/notebooks/multi_training_demo.ipynb), respectively.

Many **samples synthesized by the three models** that we compared are on [this website](http://tts.neqindi.cz/index.php?page=gallery).

Our best model supporting code-switching or voice-cloning can be downloaded [here](https://www.dropbox.com/s/hjrlg5d11er0u0c/generated_switching.pyt) and the best model trained on the whole CSS10 dataset without the ambition to do voice-cloning is available [here](https://www.dropbox.com/s/0vlz1fu2c6k1zfy/generated_training.pyt).

## Running

We are now going to show how to run the training of our multi-lingual Tacotron. We used a vocoder that is based on the WaveRNN model, see [this repository](https://github.com/Tomiinek/WaveRNN) for details, or use [our pre-trained model](#vocoding).

### :octocat: Clone repository 
```
git clone https://github.com/Tomiinek/Multilingual_Text_to_Speech.git
cd Multilingual_Text_to_Speech
```

### :mortar_board: Install python requirements 
```
pip3 install -r requirements.txt 
```

### :hourglass: Download datasets

Download the CSS10 dataset (Apache License 2.0) and our cleaned Common Voice data (Creative Commons CC0).

```
cd /project_root/data/css10
```

Visit [the CSS10 repository](https://github.com/Kyubyong/css10) and download data for all languages.
Extract downloaded archives. For example, in the case of French, you should have the following folder structure:

```
data/css10/french/lesmis/
data/css10/french/lupincontresholme/
data/css10/french/transcript.txt
```

Next, download our cleaned Common Voice dataset:

```
cd /project_root/data/comvoi_clean
```
```
wget https://www.dropbox.com/s/axoic9eoeii1zyd/clean_comvoi.tar.gz
tar -zxvf clean_comvoi.tar.gz
rm clean_comvoi.tar.gz
```

### :scroll: Prepare spectrograms

This repository provides cleaned transcripts and meta-files. You have already downloaded `.wav` files. It is handy to 
precompute spectrograms, you can run an ad-hoc script that will do it for you (it will later speed-up training):

```
cd /project_root/data/
python3 prepare_css_spectrograms.py
```

You can create the meta-file, spectrograms, and phonemicized transcripts for other datasets by applying `TextToSpeechDataset.create_meta_file`
method to the **original downloaded extracted data** (like LJ Speech, M-AILABs, etc., see `dataset/loaders.py` for supported datasets). It is then needed to split the meta-file into `train.txt` and `val.txt` files.

### :bullettrain_front: Train

Now, we can run training. See the `params/params.py` with an exhaustive description of parameters.
The `params` folder also contains **prepared parameter configurations** (like `generated_switching.json`) for the multilingual training on the whole CSS10 dataset and for the training of code-switching on the dataset that consists of Cleaned Common Voice and five languages of CSS10. 

**Train with predefined configuration** (recommended for quick start), for example:

```
PYTHONIOENCODING=utf-8 python3 train.py --hyper_parameters generated_switching
```

Please note the missing extension (`.json`).


**Or with default parameters** (default dataset is LJ Speech):
```
PYTHONIOENCODING=utf-8 python3 train.py
```

By default, **training logs** are saved into the `logs` directory. To monitor training, use Tensorboard:

```
tensorboard --logdir logs --port 6666 &
```

### :checkered_flag: Checkpointing

Checkpoints are saved into the `checkpoints` directory by default. They contain model weights, parameters, the optimizer state, and the state of the scheduler. To restore training from a checkpoint, let's say named `checkpoints/CHECKPOINT-1`, run:

```
PYTHONIOENCODING=utf-8 python3 train.py --checkpoint CHECKPOINT-1
```

## Inference

For generating spectrograms, see `synthesize.py` or interactive Colab notebooks ([here](https://colab.research.google.com/github/Tomiinek/Multilingual_Text_to_Speech/blob/master/notebooks/code_switching_demo.ipynb) and [here](https://github.com/Tomiinek/Multilingual_Text_to_Speech/blob/master/notebooks/multi_training_demo.ipynb)). An example call that uses a checkpoint `checkpoints/CHECKPOINT-1`
and that saves both the synthesized spectrogram and also the corresponding waveform vocoded using Griffin-Lim algorithm:

```
echo "01|Dies ist ein Beispieltext.|00-fr|de" | python3 synthesize.py --checkpoint checkpoints/CHECKPOINT-1 --save_spec
```

## Vocoding

We used the WaveRNN model for vocoding. You can download [WaveRNN weights](https://www.dropbox.com/s/ydep8fdzbplaamu/wavernn_weight.pyt) pre-trained on the whole CSS10 dataset.
For examples of usage, visit our interactive demos ([here](https://colab.research.google.com/github/Tomiinek/Multilingual_Text_to_Speech/blob/master/notebooks/code_switching_demo.ipynb) and [here](https://github.com/Tomiinek/Multilingual_Text_to_Speech/blob/master/notebooks/multi_training_demo.ipynb)) or [this repository](https://github.com/Tomiinek/WaveRNN).